{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33693723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from datetime import datetime, timedelta\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec151f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token OK\n"
     ]
    }
   ],
   "source": [
    "AUTH_URL    = \"http://localhost:8080/auth/public/login\"\n",
    "ORDERS_API  = \"http://localhost:8080/orders/secure/recommend\"\n",
    "PRODUCT_API = \"http://localhost:8080/product/secure/reviews/recommend\"\n",
    "USERNAME    = \"admin\"\n",
    "PASSWORD    = \"123456\"\n",
    "\n",
    "# ----- OPTIONAL DATE FILTERS (ISO format) -----\n",
    "end_date = datetime.now().date()\n",
    "start_date = end_date - timedelta(days=90)\n",
    "start_date_str = start_date.strftime(\"%Y-%m-%d\")\n",
    "end_date_str = end_date.strftime(\"%Y-%m-%d\")\n",
    "START_DATE = start_date_str          # e.g. \"2025-11-01\"\n",
    "END_DATE   = end_date_str          # e.g. \"2025-11-07\"\n",
    "\n",
    "# ========================== 1. GET TOKEN ==========================\n",
    "def get_token() -> str:\n",
    "    r = requests.post(AUTH_URL, json={\"username\": USERNAME, \"password\": PASSWORD})\n",
    "    r.raise_for_status()\n",
    "    token = r.json().get(\"token\") or r.json().get(\"accessToken\")\n",
    "    if not token:\n",
    "        raise ValueError(\"Token missing\")\n",
    "    print(\"Token OK\")\n",
    "    return token\n",
    "\n",
    "token   = get_token()\n",
    "headers = {\"Authorization\": f\"Bearer {token}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "090f06c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SAMPLE RATINGS (first 1) ---\n",
      "[{'orderId': 24, 'customerId': 5, 'totalAmount': 18810000.0, 'items': [{'variantId': 26, 'productId': 17, 'quantity': 1, 'unitPrice': 18810000.0, 'variantName': 'Máy chơi game PS5 Pro'}]}]\n"
     ]
    }
   ],
   "source": [
    "def fetch_orders() -> List[Dict[str, Any]]:\n",
    "    params = {\"status\": \"DELIVERED\"}\n",
    "    if START_DATE:\n",
    "        params[\"startDate\"] = START_DATE\n",
    "    if END_DATE:\n",
    "        params[\"endDate\"] = END_DATE\n",
    "\n",
    "    r = requests.get(ORDERS_API, headers=headers, params=params)\n",
    "    r.raise_for_status()\n",
    "    raw = r.json()\n",
    "\n",
    "    data_block = raw[\"data\"]\n",
    "    if isinstance(data_block, str):\n",
    "        data_block = json.loads(data_block)\n",
    "\n",
    "    orders = data_block[\"content\"]\n",
    "\n",
    "    orders_filtered = [\n",
    "        {\n",
    "            \"orderId\": order[\"id\"],\n",
    "            \"customerId\": order.get(\"customerId\"),\n",
    "            # \"orderDate\": order.get(\"orderDate\"),\n",
    "            \"totalAmount\": order.get(\"totalAmount\"),\n",
    "            \"items\": [\n",
    "                {\n",
    "                    \"variantId\": item.get(\"variantId\"),\n",
    "                    \"productId\": item.get(\"productId\"),\n",
    "                    \"quantity\": item.get(\"quantity\"),\n",
    "                    \"unitPrice\": item.get(\"unitPrice\"),\n",
    "                    \"variantName\": item.get(\"variantName\"),\n",
    "                }\n",
    "                for item in order.get(\"items\", [])\n",
    "            ],\n",
    "        }\n",
    "        for order in orders\n",
    "    ]\n",
    "\n",
    "    return orders_filtered\n",
    "\n",
    "orders_raw = fetch_orders()\n",
    "print(\"\\n--- SAMPLE RATINGS (first 1) ---\")\n",
    "print(orders_raw[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c2a545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- SAMPLE REVIEWS ---\n",
      "[{'reviewId': 9, 'orderId': 6, 'productId': 15, 'variantId': 18, 'customerId': 2, 'rating': 2}, {'reviewId': 10, 'orderId': 5, 'productId': 13, 'variantId': 16, 'customerId': 2, 'rating': 4}, {'reviewId': 11, 'orderId': 7, 'productId': 14, 'variantId': 22, 'customerId': 1, 'rating': 5}, {'reviewId': 12, 'orderId': 3, 'productId': 14, 'variantId': 17, 'customerId': 1, 'rating': 3}, {'reviewId': 13, 'orderId': 2, 'productId': 13, 'variantId': 16, 'customerId': 1, 'rating': 5}, {'reviewId': 14, 'orderId': 8, 'productId': 16, 'variantId': 25, 'customerId': 1, 'rating': 2}, {'reviewId': 15, 'orderId': 9, 'productId': 14, 'variantId': 20, 'customerId': 1, 'rating': 4}, {'reviewId': 16, 'orderId': 15, 'productId': 19, 'variantId': 29, 'customerId': 5, 'rating': 4}, {'reviewId': 17, 'orderId': 17, 'productId': 25, 'variantId': 37, 'customerId': 5, 'rating': 1}, {'reviewId': 18, 'orderId': 11, 'productId': 17, 'variantId': 26, 'customerId': 3, 'rating': 4}, {'reviewId': 19, 'orderId': 20, 'productId': 16, 'variantId': 25, 'customerId': 5, 'rating': 1}]\n"
     ]
    }
   ],
   "source": [
    "def fetch_reviews() -> List[Dict[str, Any]]:\n",
    "    params = {\"status\": \"DELIVERED\"}\n",
    "    if START_DATE:\n",
    "        params[\"startDate\"] = START_DATE\n",
    "    if END_DATE:\n",
    "        params[\"endDate\"] = END_DATE\n",
    "    r = requests.get(PRODUCT_API, headers=headers,params=params)\n",
    "    r.raise_for_status()\n",
    "    raw = r.json()\n",
    "    data_block = raw\n",
    "    if isinstance(data_block, str):\n",
    "        data_block = json.loads(data_block)\n",
    "\n",
    "    # reviews may be a plain list or also have \"content\"\n",
    "    reviews = data_block\n",
    "    reviews_filtered = [\n",
    "    {\n",
    "        \"reviewId\": review[\"id\"],\n",
    "        \"orderId\": review.get(\"orderId\"),\n",
    "        \"productId\": review.get(\"productId\"),\n",
    "        \"variantId\": review.get(\"variantId\"),\n",
    "        \"customerId\": review.get(\"customerId\"),\n",
    "        \"rating\": review.get(\"rating\"),\n",
    "        # \"createdAt\": review.get(\"createdAt\")\n",
    "    }\n",
    "    for review in reviews\n",
    "]\n",
    "    return reviews_filtered\n",
    "\n",
    "reviews_raw = fetch_reviews()\n",
    "print(\"\\n--- SAMPLE REVIEWS ---\")\n",
    "print(reviews_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c06d7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customerId  productId  quantity  implicit_rating source\n",
      "0           1         13         1              2.5  order\n",
      "1           1         14         3              5.0  order\n",
      "2           1         16         1              2.5  order\n",
      "3           2         13         2              5.0  order\n",
      "4           2         14         1              2.5  order\n",
      "5           2         15         2              5.0  order\n",
      "6           3         14         1              2.5  order\n",
      "7           3         17         1              2.5  order\n",
      "8           5         19         1              2.5  order\n"
     ]
    }
   ],
   "source": [
    "def process_orders(orders: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(orders)\n",
    "    # Explode items if necessary\n",
    "    if 'items' in df.columns:\n",
    "        df = df.explode('items').reset_index(drop=True)\n",
    "        items_df = pd.json_normalize(df['items'])\n",
    "        df = pd.concat([df.drop(columns=['items']), items_df], axis=1)\n",
    "    orders_agg=df.groupby(['customerId', 'productId']).agg({'quantity':'sum'}).reset_index()\n",
    "    orders_agg['implicit_rating']=np.minimum(orders_agg['quantity']*2.5,5.0)\n",
    "    orders_agg['source']='order'\n",
    "    return orders_agg\n",
    "orders_agg=process_orders(orders_raw)\n",
    "print(orders_agg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3ca3557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customerId  productId  rating  source\n",
      "0           1         13     5.0  review\n",
      "1           1         14     4.0  review\n",
      "2           1         16     2.0  review\n",
      "3           2         13     4.0  review\n",
      "4           2         15     2.0  review\n",
      "5           5         19     4.0  review\n"
     ]
    }
   ],
   "source": [
    "def process_reviews(reviews: List[Dict[str, Any]]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(reviews)\n",
    "    reviews_agg=df.groupby(['customerId', 'productId']).agg({'rating':'mean'}).reset_index()\n",
    "    reviews_agg['source']='review'\n",
    "    return reviews_agg\n",
    "reviews_agg=process_reviews(reviews_raw)\n",
    "print(reviews_agg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbf13ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw Merged:\n",
      "    customerId  productId  quantity  implicit_rating source_x  rating source_y\n",
      "0           1         13         1              2.5    order     5.0   review\n",
      "1           1         14         3              5.0    order     4.0   review\n",
      "2           1         16         1              2.5    order     2.0   review\n",
      "3           2         13         2              5.0    order     4.0   review\n",
      "4           2         14         1              2.5    order     NaN      NaN\n",
      "5           2         15         2              5.0    order     2.0   review\n",
      "6           3         14         1              2.5    order     NaN      NaN\n",
      "7           3         17         1              2.5    order     NaN      NaN\n",
      "8           5         19         1              2.5    order     4.0   review\n"
     ]
    }
   ],
   "source": [
    "merged = pd.merge(orders_agg, reviews_agg, on=['customerId', 'productId'], how='outer')\n",
    "print(\"Raw Merged:\\n\", merged)  # Check for NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e0d30a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactions DF:\n",
      "    customerId  productId  final_rating  source  quantity\n",
      "0           1         13           5.0  review         1\n",
      "1           1         14           4.0  review         3\n",
      "2           1         16           2.0  review         1\n",
      "3           2         13           4.0  review         2\n",
      "4           2         14           2.5   order         1\n",
      "5           2         15           2.0  review         2\n",
      "6           3         14           2.5   order         1\n",
      "7           3         17           2.5   order         1\n",
      "8           5         19           4.0  review         1\n"
     ]
    }
   ],
   "source": [
    "merged['final_rating'] = np.where(merged['rating'].notna(), merged['rating'], merged['implicit_rating'])\n",
    "merged['source']=merged['source_y'].fillna(merged['source_x'])\n",
    "interactions_df=merged[['customerId', 'productId', 'final_rating', 'source','quantity']].copy()\n",
    "interactions_df=interactions_df.dropna(subset=['final_rating'])\n",
    "print(\"Interactions DF:\\n\", interactions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ce7b95a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User-Item Matrix:\n",
      " productId    13   14   15   16   17   19\n",
      "customerId                              \n",
      "1           5.0  4.0  0.0  2.0  0.0  0.0\n",
      "2           4.0  2.5  2.0  0.0  0.0  0.0\n",
      "3           0.0  2.5  0.0  0.0  2.5  0.0\n",
      "5           0.0  0.0  0.0  0.0  0.0  4.0\n"
     ]
    }
   ],
   "source": [
    "user_item_matrix = interactions_df.pivot_table(index='customerId', columns='productId', values='final_rating',fill_value=0)\n",
    "print(\"User-Item Matrix:\\n\", user_item_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74e87f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 14, 15, 16, 17, 19]\n",
      "Sample Fake:\n",
      "    customerId  productId  final_rating     source  quantity\n",
      "0           6         19           5.0  synthetic         1\n",
      "1           6         15           1.0  synthetic         1\n",
      "2           7         13           2.2  synthetic         1\n",
      "3           7         19           2.6  synthetic         1\n",
      "4           7         15           1.8  synthetic         1\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(44)  # Reproducible fakes\n",
    "\n",
    "print( user_item_matrix.columns.tolist())\n",
    "products=user_item_matrix.columns.tolist()\n",
    "fake_ids=np.arange(6, 30)\n",
    "fake_data = []\n",
    "real_avg_ratings = interactions_df.groupby('productId')['final_rating'].mean().to_dict()\n",
    "for cust_id in fake_ids:\n",
    "    num_interactions = np.random.randint(2, 5)  # 2-4 products per cust\n",
    "    selected_products = np.random.choice(products, size=num_interactions, replace=False)\n",
    "    for prod_id in selected_products:\n",
    "        real_avg = real_avg_ratings.get(prod_id, 3.0)  \n",
    "        rating = np.clip(np.random.normal(real_avg, 1.0), 1, 5)\n",
    "        \n",
    "        quantity = np.random.randint(1, 2) \n",
    "        \n",
    "        fake_data.append({\n",
    "            'customerId': cust_id,\n",
    "            'productId': prod_id,\n",
    "            'final_rating': round(rating, 1), \n",
    "            'source': 'synthetic',\n",
    "            'quantity': quantity\n",
    "        })\n",
    "\n",
    "fake_df = pd.DataFrame(fake_data)\n",
    "print(\"Sample Fake:\\n\", fake_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9600e902",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   customerId  productId  final_rating     source  quantity\n",
      "0           1         13           5.0     review         1\n",
      "1           1         14           4.0     review         3\n",
      "2           1         16           2.0     review         1\n",
      "3           2         13           4.0     review         2\n",
      "4           2         14           2.5      order         1\n",
      "5           2         15           2.0     review         2\n",
      "6           3         14           2.5      order         1\n",
      "7           3         17           2.5      order         1\n",
      "8           5         19           4.0     review         1\n",
      "9           6         19           5.0  synthetic         1\n",
      "Clean Augmented: 79 pairs\n",
      "source\n",
      "order         3\n",
      "review        6\n",
      "synthetic    70\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "augmented_df = pd.concat([interactions_df, fake_df], ignore_index=True)\n",
    "print(augmented_df.head(10))\n",
    "augmented_df=augmented_df.drop_duplicates(subset=['customerId', 'productId'], keep='last')\n",
    "augmented_df=augmented_df.sort_values(['customerId', 'productId']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Clean Augmented: {len(augmented_df)} pairs\")\n",
    "print(augmented_df.groupby('source').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "052f2337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented Matrix Shape: (28, 6)\n",
      "productId    13   14   15   16   17   19\n",
      "customerId                              \n",
      "1           5.0  4.0  0.0  2.0  0.0  0.0\n",
      "2           4.0  2.5  2.0  0.0  0.0  0.0\n",
      "3           0.0  2.5  0.0  0.0  2.5  0.0\n",
      "5           0.0  0.0  0.0  0.0  0.0  4.0\n",
      "6           0.0  0.0  1.0  0.0  0.0  5.0\n"
     ]
    }
   ],
   "source": [
    "augmented_matrix= augmented_df.pivot_table(\n",
    "    index='customerId',\n",
    "    columns='productId',\n",
    "    values='final_rating',\n",
    "    fill_value=0)\n",
    "print(\"Augmented Matrix Shape:\", augmented_matrix.shape)\n",
    "print(augmented_matrix.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28b45505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Sparsity: 53.0%\n",
      "Saved! Use augmented_df or augmented_matrix for modeling.\n"
     ]
    }
   ],
   "source": [
    "sparsity=1.0-(augmented_matrix>0).sum().sum()/(augmented_matrix.shape[0]*augmented_matrix.shape[1])\n",
    "print(f\"New Sparsity: {sparsity*100:.1f}%\")\n",
    "augmented_df.to_csv(\"augmented_interactions.csv\", index=False)\n",
    "print(\"Saved! Use augmented_df or augmented_matrix for modeling.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9ad37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler=MinMaxScaler()\n",
    "normalized_matrix = scaler.fit_transform(augmented_matrix)\n",
    "normalized_df=pd.DataFrame(normalized_matrix, index=augmented_matrix.index, columns=augmented_matrix.columns)\n",
    "print(\"Normalized Sample (Cust1):\\n\", normalized_df.loc[1].head())  # Ratings 0-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12bd9281",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_array= normalized_df.values\n",
    "similarity_matrix = cosine_similarity(norm_array)\n",
    "sim_df=pd.DataFrame(similarity_matrix, index=augmented_matrix.index, columns=augmented_matrix.index)\n",
    "print(\"Similarity Sample (Cust1 to others):\\n\", sim_df.loc[1].sort_values(ascending=False).head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083032fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(customer_id,matrix=augmented_matrix,sim_df=sim_df,k=5,n=3):\n",
    "    if customer_id not in matrix.index:\n",
    "        return \"New customer: Recommend popular products (e.g., top avg rated).\"\n",
    "    similar_custs = sim_df.loc[customer_id].sort_values(ascending=False).iloc[1:k+1].index.tolist()\n",
    "    print(f\"Top {k} similar to {customer_id}: {similar_custs}\")\n",
    "\n",
    "    customer_ratings= matrix.loc[customer_id]\n",
    "    unseen=customer_ratings[customer_ratings==0].index.tolist()\n",
    "    if not unseen:\n",
    "        return \"No new recs! All seen!\"\n",
    "    \n",
    "    predictions = {}\n",
    "    for prod in unseen:\n",
    "        weighted_sum=0\n",
    "        sim_sum=0\n",
    "        for sim_cust in similar_custs:\n",
    "            if(matrix.loc[sim_cust,prod]>0):\n",
    "                sim_score=sim_df.loc[customer_id,sim_cust]\n",
    "                rating=matrix.loc[sim_cust,prod]\n",
    "                weighted_sum += sim_score * rating\n",
    "                sim_sum += sim_score\n",
    "        if sim_sum>0:\n",
    "            predictions[prod]=weighted_sum/sim_sum\n",
    "        else:\n",
    "            predictions[prod]=0\n",
    "    \n",
    "    recs=sorted(predictions.items(), key=lambda x: x[1], reverse=True)[:n]\n",
    "    return [(prod, round(score,2)) for prod, score in recs]\n",
    "\n",
    "# Test on real cust\n",
    "recs = get_recommendations(1, n=2)\n",
    "print(f\"Recs for Cust1: {recs}\")\n",
    "\n",
    "# Popular fallback (for cold starts)\n",
    "popular = augmented_matrix.mean().sort_values(ascending=False).head(3)\n",
    "print(\"Global Popular:\", popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "83b99d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created! Layers: RecModel(\n",
      "  (user_emb): Embedding(29, 32)\n",
      "  (item_emb): Embedding(17, 32)\n",
      "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Sample forward pass:\n",
      "Predicted rating: -0.09\n",
      "\n",
      "User map (sample): {6: 0, 7: 1, 8: 2, 9: 3, 10: 4}...\n",
      "Item map keys (sample): [13, 14, 15, 16, 17]...\n",
      "\n",
      "Training data ready: 70 samples\n",
      "Sample batch: [(0, 5, 1.0), (0, 2, 0.2), (1, 0, 0.44000000000000006)]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "\n",
    "class RecModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, emb_dim=32):\n",
    "        super(RecModel, self).__init__()\n",
    "        self.user_emb=nn.Embedding(num_users, emb_dim)\n",
    "        self.item_emb=nn.Embedding(num_items, emb_dim)\n",
    "        self.fc=nn.Linear(emb_dim*2, 1)\n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_vecs=self.user_emb(user_ids)\n",
    "        item_vecs=self.item_emb(item_ids)\n",
    "        x=torch.cat([user_vecs, item_vecs], dim=-1)\n",
    "        out=self.fc(x)\n",
    "        return out.squeeze()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Your existing model test (keep it)\n",
    "    num_users = 29  # Updated: Enough for fake_ids up to 29\n",
    "    num_items = 17  # From your Step 1\n",
    "    model = RecModel(num_users, num_items)\n",
    "    print(\"Model created! Layers:\", model)\n",
    "    print(\"Sample forward pass:\")\n",
    "    sample_u = torch.tensor([0])  # User index 0\n",
    "    sample_i = torch.tensor([2])  # Product index 2\n",
    "    pred = model(sample_u, sample_i)\n",
    "    print(f\"Predicted rating: {pred.item():.2f}\")\n",
    "\n",
    "\n",
    "    # NEW: Create mappings (real_id → internal_idx) - Updated for larger IDs\n",
    "    unique_customers = sorted(fake_df['customerId'].unique())  # Only fakes for now; add reals later\n",
    "    unique_products = sorted(set(products))  # All possible\n",
    "    user_map = {cust: idx for idx, cust in enumerate(unique_customers)}\n",
    "    item_map = {prod: idx for idx, prod in enumerate(unique_products)}\n",
    "    print(f\"\\nUser map (sample): {dict(list(user_map.items())[:5])}...\")  # First 5\n",
    "    print(f\"Item map keys (sample): {list(item_map.keys())[:5]}...\")\n",
    "\n",
    "    # NEW: Prepare training data (list of (u_idx, i_idx, normalized_rating))\n",
    "    train_data = []\n",
    "    for _, row in fake_df.iterrows():  # Train on fakes only for test\n",
    "        u_idx = user_map[row['customerId']]\n",
    "        i_idx = item_map[row['productId']]\n",
    "        r_norm = row['final_rating'] / 5.0  # Normalize to [0,1]\n",
    "        train_data.append((u_idx, i_idx, r_norm))\n",
    "    print(f\"\\nTraining data ready: {len(train_data)} samples\")\n",
    "    print(\"Sample batch:\", train_data[:3])  # First 3 tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "79afcc04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training the Model (Fixed) ===\n",
      "Epoch 1/20, Train Loss: 0.4348, Val Loss: 0.2978\n",
      "Training completed! Model learned embeddings.\n",
      "\n",
      "=== Adding Negative Sampling for Better Recs ===\n",
      "Added 140 negatives-model now penalizes unseen as low-rated\n",
      "Epoch 2/20, Train Loss: 0.4146, Val Loss: 0.1365\n",
      "Training completed! Model learned embeddings.\n",
      "\n",
      "=== Adding Negative Sampling for Better Recs ===\n",
      "Added 140 negatives-model now penalizes unseen as low-rated\n",
      "Epoch 3/20, Train Loss: 0.4150, Val Loss: 0.0664\n",
      "Training completed! Model learned embeddings.\n",
      "\n",
      "=== Adding Negative Sampling for Better Recs ===\n",
      "Added 140 negatives-model now penalizes unseen as low-rated\n",
      "Epoch 4/20, Train Loss: 0.4208, Val Loss: 0.0783\n",
      "Training completed! Model learned embeddings.\n",
      "\n",
      "=== Adding Negative Sampling for Better Recs ===\n",
      "Added 140 negatives-model now penalizes unseen as low-rated\n",
      "Epoch 5/20, Train Loss: 0.4085, Val Loss: 0.0454\n",
      "Training completed! Model learned embeddings.\n",
      "\n",
      "=== Adding Negative Sampling for Better Recs ===\n",
      "Added 140 negatives-model now penalizes unseen as low-rated\n",
      "Epoch 6/20, Train Loss: 0.4102, Val Loss: 0.0414\n",
      "Training completed! Model learned embeddings.\n",
      "\n",
      "=== Adding Negative Sampling for Better Recs ===\n",
      "Added 140 negatives-model now penalizes unseen as low-rated\n",
      "Epoch 7/20, Train Loss: 0.4119, Val Loss: 0.0758\n",
      "Training completed! Model learned embeddings.\n",
      "\n",
      "=== Adding Negative Sampling for Better Recs ===\n",
      "Added 140 negatives-model now penalizes unseen as low-rated\n",
      "Epoch 8/20, Train Loss: 0.4069, Val Loss: 0.0619\n",
      "Training completed! Model learned embeddings.\n",
      "\n",
      "=== Adding Negative Sampling for Better Recs ===\n",
      "Added 140 negatives-model now penalizes unseen as low-rated\n",
      "Epoch 9/20, Train Loss: 0.4161, Val Loss: 0.0640\n",
      "Early stopping at epoch 9\n",
      "Test Prediction for Cust6, Prod15: 1.00\n",
      "Ground truth rating: 1.0\n",
      "Error: 0.00\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== Training the Model (Fixed) ===\")\n",
    "optimizer = Adam(model.parameters(), lr=0.01)  # Optimizer\n",
    "criterion = nn.MSELoss()  # Loss: Mean Squared Error\n",
    "\n",
    "num_epochs = 20\n",
    "\n",
    "train_size=int(0.9*len(train_data))\n",
    "val_size=len(train_data)-train_size\n",
    "train_split,val_split=torch.utils.data.random_split(train_data,[train_size,val_size])\n",
    "\n",
    "train_tensors=[(torch.tensor([u],dtype=torch.long),torch.tensor([i],dtype=torch.long),\n",
    "                torch.tensor([r],dtype=torch.long)) for u,i,r in train_split]\n",
    "val_tensors = [(torch.tensor([u], dtype=torch.long), torch.tensor([i], dtype=torch.long),\n",
    "                 torch.tensor([r], dtype=torch.float)) for u, i, r in val_split]\n",
    "\n",
    "def build_dataset(split):\n",
    "    users, items, ratings = zip(*[\n",
    "        (torch.tensor(u, dtype=torch.long),\n",
    "         torch.tensor(i, dtype=torch.long),\n",
    "         torch.tensor(float(r), dtype=torch.float))\n",
    "        for (u, i, r) in split\n",
    "    ])\n",
    "\n",
    "    return TensorDataset(\n",
    "        torch.stack(users),\n",
    "        torch.stack(items),\n",
    "        torch.stack(ratings)\n",
    "    )\n",
    "\n",
    "train_dataset = build_dataset(train_split)\n",
    "val_dataset = build_dataset(val_split)\n",
    "\n",
    "batch_size = min(8, len(train_dataset))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=len(val_dataset), shuffle=False)\n",
    "\n",
    "best_val_loss=float('inf')\n",
    "patience=3\n",
    "no_improve=0\n",
    "\n",
    "model.train()  # Enable training mode (gradients on)\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for batch in train_loader:\n",
    "        users,items,targets=batch\n",
    "        pred = model(users, items).unsqueeze(1)\n",
    "        loss = criterion(pred, targets.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss+=loss.item()+len(batch)\n",
    "    avg_train_loss=train_loss/len(train_dataset)\n",
    "\n",
    "    model.eval()\n",
    "    val_loss=0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            users,items,targets=batch\n",
    "            pred = model(users, items).unsqueeze(1)\n",
    "            val_loss+=criterion(pred,targets.unsqueeze(1)).item()*len(batch)\n",
    "    avg_val_loss=val_loss/len(val_dataset)\n",
    "    model.train()\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    if avg_val_loss<best_val_loss:\n",
    "        best_val_loss=avg_val_loss\n",
    "        no_improve=0\n",
    "    else:\n",
    "        no_improve+=1\n",
    "        if no_improve>= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "    model.eval()\n",
    "    print(\"Training completed! Model learned embeddings.\")\n",
    "\n",
    "    print(\"\\n=== Adding Negative Sampling for Better Recs ===\")\n",
    "    neg_samples=[]\n",
    "    num_neg=len(train_data)*2\n",
    "    for _ in range(num_neg):\n",
    "        u_idx=np.random.randint(0,num_users)\n",
    "        i_idx=np.random.randint(0,num_items)\n",
    "        if(u_idx,i_idx,0.0) not in train_data:\n",
    "            neg_samples.append((u_idx,i_idx,0.0))\n",
    "    \n",
    "    model.train()\n",
    "    for u,i,r in neg_samples[:100]:\n",
    "        pred=model(torch.tensor([u],dtype=torch.long),torch.tensor([i],dtype=torch.long)).unsqueeze(0)\n",
    "        loss=criterion(pred,torch.tensor([r],dtype=torch.float))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval()\n",
    "    print(f\"Added {len(neg_samples)} negatives-model now penalizes unseen as low-rated\")\n",
    "\n",
    "test_u=6\n",
    "test_i=15\n",
    "u_idx=user_map[test_u]\n",
    "i_idx=item_map[test_i]\n",
    "with torch.no_grad():\n",
    "    pred_norm=model(torch.tensor([u_idx],dtype=torch.long),\n",
    "                     torch.tensor([i_idx],dtype=torch.long)).item()\n",
    "    pred_rating=pred_norm*5.0\n",
    "    pred_rating=max(1.0, min(5.0, pred_rating))\n",
    "print(f\"Test Prediction for Cust{test_u}, Prod{test_i}: {pred_rating:.2f}\")\n",
    "if not fake_df[(fake_df['customerId']==test_u) & (fake_df['productId']==test_i)].empty:\n",
    "    true_rating = fake_df[(fake_df['customerId']==test_u) & (fake_df['productId']==test_i)]['final_rating'].iloc[0]\n",
    "    print(f\"Ground truth rating: {true_rating}\")\n",
    "    print(f\"Error: {abs(pred_rating - true_rating):.2f}\")\n",
    "else:\n",
    "    print(\"No exact match—model generalizes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e51a44d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating Recommendations ===\n",
      "Top 3 recommendations for Customer 4: \n",
      "Product 13: Score 4.177777777777778\n",
      "Product 19: Score 3.7999999999999994\n",
      "Product 14: Score 3.3\n",
      "Product 17: Score 2.835714285714286\n",
      "Product 15: Score 2.2714285714285714\n",
      "Product 16: Score 1.907142857142857\n"
     ]
    }
   ],
   "source": [
    "def get_recommendations(model,user_map,item_map,df,customer_id,n=3):\n",
    "    if customer_id not in user_map:\n",
    "        popular=df.groupby('productId')['final_rating'].mean().sort_values(ascending=False).head(n)\n",
    "        return[{'product_id':int(pid),'score':float(score)} for pid,score in popular.items()]\n",
    "    u_idx=user_map[customer_id]\n",
    "    seen_products=df[df['customerId']==customer_id]['productId'].unique()\n",
    "    all_products=list(item_map.keys())\n",
    "    unseen=[p for p in all_products if p not in seen_products]\n",
    "\n",
    "    if not unseen:\n",
    "        return []\n",
    "    \n",
    "    predictions = {}\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for prod in unseen:\n",
    "            i_idx=item_map[prod]\n",
    "            pred_norm=model(torch.tensor([u_idx],dtype=torch.long),\n",
    "                            torch.tensor([i_idx],dtype=torch.long)).item()\n",
    "            pred_rating=pred_norm*5.0\n",
    "            pred_rating=max(1.0,min(5.0,pred_rating))\n",
    "            predictions[int(prod)]=pred_rating\n",
    "    \n",
    "    top_recs=sorted(predictions.items(),key=lambda x:x[1],reverse=True)[:n]\n",
    "    return [{'product_id':prod,\"score\":round(score,2)} for prod,score in top_recs]\n",
    "\n",
    "print(\"\\n=== Generating Recommendations ===\")\n",
    "recs=get_recommendations(model,user_map,item_map,fake_df,customer_id=3,n=6)\n",
    "print(f\"Top 3 recommendations for Customer 4: \")\n",
    "for rec in recs:\n",
    "    print(f\"Product {rec['product_id']}: Score {rec['score']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
